TODO List:
- [ ] Architecture

Modern data is moving more towards the unstructured type and huge, conventional data processing option like RDBMS is even more difficult, time-consuming and costly. But Hadoop’s MapReduce Programming is much effective, safer, and quicker in processing large datasets of even terabytes or petabytes.


- MapReduce is a programming model or pattern for distributed computing within the Hadoop framework
- Performs the processing of large distributed data sets in a parallel manner
- Used to access big data stored in the Hadoop File System (HDFS)
- Used for large-scale data processing by dividing the data into smaller chunks, then processing those chunks in a parallel manner, and finally combining the results to get a final output.
- Core component of the Hadoop framework
- Based on Java

# How does MapReduce work?

![[mapreduce-data-flow.png]]

![[mapreduce-phases.png]]

1. **Input Reader:**
	- Generally the input data is stored in the form of file or directory in Hadoop file system (HDFS).
	- The input reader reads the upcoming input data and splits it into smaller data blocks containing key-value pairs as (k, v), where the key represents the offset address of each record and the value represents the entire record content.
	- Each data block is then assigned to a mapper for processing. The Hadoop framework decides how many mappers to use, based on the size of the data to be processed and the memory block available on each mapper server.
	
2. **Map:** 
	- The Map Phase is the first Phase of MapReduce which processes the data in parallel across multiple worker nodes.
	- Each worker node has its own Map function which applies to the input data.
	- The Map function accepts key-value pairs as input as (k, v), where the key represents the offset address of each record and the value represents the entire record content and gives an output in the form of intermediate key-value pairs as (k',  v').
	- Each worker node writes the output to temporary storage.
	- The primary (master) node ensures that only a single copy of the redundant input data is processed.
	- If a node fails while processing the data, the work gets shifted towards other nodes, so there can’t be any interruption during processing.
	
3. **Partition, Shuffle and Sort:**
	- The Shuffle and Sort Phase occur in between the Map and Reduce Phases.
	- The partitioner in the Shuffle and Sort Phase divides the intermediate data generated by the map tasks into distinct subsets based on their keys.
	- The partitioner determines the hash value for the key, resulting from the mapper, and assigns a partition based on this hash value.
	- Each partition corresponds to a specific reduce task.
	- Partitioning ensures that all the intermediate key-value pairs with the same key ends up in the same partition, which will be processed by the same reduce task.
	- The shuffle phase involves transferring the intermediate data from the map tasks over the network from the map nodes to the reduce nodes based on partition.
	- The shuffle phase ensures that each reduce task receives the intermediate data it needs for processing.
	- Once the data reaches the reduce nodes, it needs to be sorted based on the key.
	- The sorting phase ensures that all key-value pairs with the same key are grouped together which allows the reduce tasks to efficiently process the data.
	- The output of this phase will be key-value pairs again as (k, v[ ]).

>[!note]
>- If Sorting is optional for any task, it would be beneficial not to Sort the key-value pairs because Sorting is time-consuming.
>- In MapReduce, minimizing the network traffic, using efficient sorting algorithms and proper partitioning strategies are very crucial operations to optimize the job performance


4. **Reduce:** 
	- The Reduce Phase is the final Phase of the MapReduce job and it cannot start while a map phase is still in progress.
	- The output of the Shuffle and Sort phase (k, v[]) will be the input of the Reducer phase.
	- The Reduce Phase is responsible for performing the user-defined Reduce Function on the values associated with the key.
	- The Reduce Function is designed to run on multiple worker nodes which process each group of key-value pairs input data, in parallel to produce key-value pairs as output
	- The Reduce Function can perform any operation on the values associated with the keys such as sum, average and many other aggregate functions and generates the corresponding output.
	- The final output is then written into a single file in an output directory of HDFS.
	- Unlike the Map function which is mandatory to filter and sort the initial data, the Reduce function is optional.
	
5. **Combiner:**
	- It is an optional phase in the MapReduce model.
	- The combiner is a reducer that runs individually on each mapper server.
	- It is used to optimize the performance of MapReduce jobs.
	- It reduces the output data of each mapper even further to a simplified form by applying aggregate function before passing it downstream.
	- This makes shuffling and sorting work easier and quicker as there is less amount of data to work with.
	- Often, the combiner class is set to the reducer class itself, due to the cumulative and associative functions in the reduce function. However, if needed, the combiner can be a separate class as well.


>[!note]
>The speed of MapReduce is dominated by the slowest task. So, to up the speed, a new mapper will work on the same dataset at the same time. Whichever completes the task first is considered as the final output and the other one is killed. It is an optimization technique.

# Benefits of MapReduce

Here are the benefits of MapReduce mentioned below.
#### 1. Fault-tolerance

- During the middle of a map-reduce job, if a machine carrying a few data blocks fails architecture handles the failure.
- It considers replicated copies of the blocks in alternate machines for further processing.

#### 2. Resilience

- Each node periodically updates its status to the master node.
- If a slave node doesn’t send its notification, the master node reassigns the currently running task of that slave node to other available nodes in the cluster.

#### 3. Quick

- Data processing is quick as MapReduce uses HDFS as the storage system.
- MapReduce takes minutes to process terabytes of unstructured large volumes of data.

#### 4. Parallel Processing

- MapReduce tasks process multiple chunks of the same datasets in-parallel by dividing the tasks.
- This gives the advantage of task completion in less time.

#### 5. Availability

- Multiple replicas of the same data are sent to numerous nodes in the network.
- Thus, in case of any failure, other copies are readily available for processing without any loss.

#### 6. Scalability

- Hadoop is a highly scalable platform.
- Traditional RDBMS systems are not scalable according to the increase in data volume.
- MapReduce lets you run applications from a huge number of nodes, using terabytes and petabytes of data.

#### 7. Cost-effective

- Hadoop’s scale-out feature along with MapReduce programming lets you store and process data in a very effective and affordable manner.
- Cost savings can be massive as figures of hundreds for terabytes of data.


# A Map Reduce Example

MapReduce logic to find the word count on an array of words can be shown as below:

![[mapreduce-word-count.png]]


```
input = ["This is not good", "It is too good"]
```


- The mapper phase tokenizes the input array of words into the ‘n’ number of words to give the output as (k, v), ie, (This, 1).
- Shuffle and Sort accept the mapper (k, v) output and group all values according to their keys as (k, v[]). i.e. (good, \[1, 1]).
- The Reducer phase accepts Shuffle and sort output and gives the aggregate of the values (good, \[1+1]), corresponding to their keys. i.e. (good, 2).




